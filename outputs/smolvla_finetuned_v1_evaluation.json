{
    "q0a": "/home/olin/Robotics/Projects/LeRobot/lerobot/outputs/train/2025-08-18/13-40-25_smolvla/checkpoints/last/pretrained_model\n\nTB: olingoudey/smolvla_finetuned_v1\n",
    "q0b": "\nGeneral: that of smolvla_bas\n\nSpecific: that of olingoudey/put_stuffed_animal_in_bowl\nNo recoveries, one initial position of the bowl.",
    "q0c": "Living room I guess. SO-101 in the living room.\n\n",
    "q1a": "Specific training is realistic in computational quality (it's real), but it is too simple, only a couple of objects.\n\nCompared to the target environment - it's \"toy\". Not my actual living room - it's got a towel backdrop and constrained camera angles.\n",
    "q1b": "\nSpot on except for the intrusive exocentric cameras. Agent might also be too small, too weak (not sure), or too morphological simple (only two digits), and thus restricted on some level to the dexterity-intelligence of a crab.\n",
    "q1c": "\nNo abstractions, just the real simple object.",
    "q1d":
    
    "Does the object generalize? Seems dependent on the object looking like the stuffed animal, but is less likely to do it if the object is called something else. Does the skill generalize? Can only do its finetuned task it seems. Does the language generalize (is the skill free from the language)? Skill is free from language in that plenty of instructions yield nothing effective. Does more decorated language ease (or just affect) the skill?\n\nDoes the object generalize without the corresponding linguistic conditions (is it free from language)? Kind of - 'do something useful' is no good, but subbing in 'the object' is kinda fine. It doesn't seem to care if the stuffed animal is called a 'cup'.\n\nDo the camera angles generalize?\n\nWhat has fine-tuning done? Can the bowl move? It can generalize! A glass ('glass') works. Can the object move? It can recover and (50%) take approximately the initial positions trained on. Are there 1+ \"skills\" (does stable language-freedom count as a skill? - Yes, but 'do nothing', 'wait for further instruction', 'don't move' all are ineffective.) No. Only the one?\n\nFuture work: Can the policy learn __inclusive avoidance with negation__ to avoid dropping the object in a place which it cannot reach?",
    "q2a": "Very strong. The policy can recover in ways not seen in the finetuning dataset.",
    "q2b": "\nAgain, solid. Presumably the policy can run in a loop with appropriate resetting of the environment (or without, if it generalizes (TBD)).",
    "q2c": "Could potentially look into SayCan? But SayCan I don't think synergizes with language generality - more with an instruction set.\n\nIf the user knows an instruction set and it doesn't generalize, one could hook up STT. One could consider adding layers around the VLA L input.\n\nOr, one could program a loop to \"X instruction\", \"Y instruction\", \"Z instruction\", \"X instruction\", ... Depending on whether conditionals are understood.\n\nIf there is an automatic stimulus (and even with user-only stimulus), one would have to consider a \"done\" signal from the robot. Looping, I presume, is the 'status quo' (confirmed).\n",
    "q3a": "\nPretty useless\n\n",
    "q3b": "\nPretty useless\n",
    "q3c": "\n95% down time. 5% noticing something and (e.g.) putting it away.\n\nOr, linked up to an easy UI.\n\nThe above ideas do not really leverage language generalization, but rather use the policy as a suitable replacement for traditional techniques that tried to do the same (RL to pick something up, motion-planning, etc). But more than suitable - it recovers on the fly, it its naturally collaborative and collision avoidant.\n\nChatbot server style: Have it somehow default to non-activity and just have it \"do things\" in a station. Assumes language generality.\n\nA more \"respectful\" approach is to give the robot a life. Calculate the price/day, give it a station. Give it a \"sleep schedule\". Upload new versions in \"sleep\". (Give it ego-centric cameras.) Give it pals."
}
